\chapter{Data Center Network (DCN) Architectures}
\author{Guido Maier}

This chapter provides an in-depth exploration of the network infrastructures that underpin modern data centers. We will trace the evolution of application architectures and their impact on network demands, contrast classical and modern DCN topologies, and examine the state-of-the-art solutions deployed by hyperscale providers.

\section{Evolution of Network Demands}
The architecture of data center networks has evolved in response to a fundamental shift in application design and traffic patterns.
\begin{description}
    \item[Monolithic to Microservices] Applications have transitioned from single-server, monolithic structures to distributed, multi-unit web applications and, more recently, to fine-grained microservices. This has moved the computational load from the client to the server side, hosted within large data centers.
    \item[The Rise of East-West Traffic] This architectural shift has led to a dramatic increase in server-to-server communication \textit{within} the data center. This \textbf{East-West traffic}, driven by functions like storage replication, distributed data processing (e.g., MapReduce), and VM migration, now constitutes the vast majority (often >75\%) of all traffic, dwarfing the traditional client-to-data-center (\textbf{North-South}) traffic.
\end{description}
This new reality demands a network fabric optimized for high-bandwidth, low-latency communication between any two servers in the facility.

\section{Fundamental DCN Concepts}
\subsection{Bisection Bandwidth}
The most critical performance metric for a modern DCN is its \textbf{bisection bandwidth}. This is defined as the available bandwidth across the narrowest cut that divides the network's servers into two equal halves. A high bisection bandwidth ensures that the network is not a bottleneck and can support intense, all-to-all communication patterns without performance degradation. Scaling this bandwidth effectively is the primary challenge in DCN design.

\subsection{Classes of DCN Architectures}
DCNs can be broadly classified into three categories:
\begin{itemize}
    \item \textbf{Switch-centric:} The most common approach, using dedicated hardware switches to perform packet forwarding.
    \item \textbf{Server-centric:} An alternative model that uses servers with multiple Network Interface Cards (NICs) to act as both compute nodes and forwarding elements (e.g., the CamCube 3D Torus).
    \item \textbf{Hybrid:} Architectures that combine switches and servers for packet forwarding (e.g., DCell, BCube).
\end{itemize}

\section{The Classical Three-Tier Architecture}
The traditional approach to enterprise and data center networking is a hierarchical, three-tier topology.
\begin{description}
    \item[Access Layer] The lowest tier, where servers connect to the network, typically via a \textbf{Top-of-Rack (ToR)} switch located in each server rack.
    \item[Aggregation Layer] The middle tier, which connects multiple access switches, aggregating their traffic and providing a redundant path upwards.
    \item[Core Layer] The high-speed network backbone, connecting all aggregation switches and handling routing to the external internet.
\end{description}
While simple and logical, this model has significant drawbacks for large-scale, East-West heavy workloads. It becomes prohibitively expensive, as the aggregation and core layers require large, powerful, and costly chassis-based switches. Furthermore, it is prone to oversubscription and bottlenecks, limiting the crucial bisection bandwidth.

\section{The Modern Standard: Leaf-Spine (Clos) Architecture}
To address the shortcomings of the three-tier model, modern DCNs are built using a \textbf{Leaf-Spine} topology, a practical implementation of the non-blocking \textbf{Clos network} developed for telephony.
\begin{description}
    \item[Two-Tier Topology] The architecture consists of only two layers of switches:
    \begin{itemize}
        \item \textbf{Leaf Switches:} The access layer, where servers connect.
        \item \textbf{Spine Switches:} A fully interconnected core layer.
    \end{itemize}
    \item[Full-Mesh Interconnection] Every leaf switch is connected to every spine switch in the fabric. This ensures that any communication between two servers in different racks travels a path of Leaf $\rightarrow$ Spine $\rightarrow$ Leaf.
\end{description}
\subsection{Advantages of Leaf-Spine}
\begin{itemize}
    \item \textbf{High, Predictable Bisection Bandwidth:} The full-mesh design provides uniform, non-blocking bandwidth between all endpoints.
    \item \textbf{Scalability:} The network can be scaled out horizontally by adding more spine switches to increase bandwidth or by adding more leaf switches to increase the server port count.
    \item \textbf{Cost-Effectiveness:} The architecture is typically built with a large number of identical, fixed-configuration "commodity" switches, which is significantly cheaper than using a few massive, monolithic core switches.
    \item \textbf{Low and Consistent Latency:} The hop count between any two servers is always the same, leading to predictable application performance.
\end{itemize}

\section{Scaling to Warehouse-Scale Computers (WSCs)}
\subsection{The Fat-Tree Architecture}
For hyperscale data centers with hundreds of thousands of servers, the Leaf-Spine concept is extended into a multi-stage Clos network, often called a \textbf{Fat-Tree}.
\begin{itemize}
    \item \textbf{Points of Delivery (PODs):} The network is built from repeatable units called PODs, where each POD is essentially a self-contained Leaf-Spine network.
    \item \textbf{Super-Spine Layer:} A third tier of switches (a "super-spine" or core layer) is introduced to provide full interconnection between all the PODs.
\end{itemize}
This modular, pod-based approach, famously used by Microsoft and Amazon, allows for massive scalability while preserving the benefits of the Clos topology. For example, Google's Jupiter datacenter network, a multistage Clos design, achieved a bisection bandwidth of over 1.3 Pbit/s using commodity switch components.

\subsection{The Next Frontier: Optical Circuit Switching (OCS)}
Leading-edge data centers are evolving beyond purely electrical packet-switched networks. Given that inter-block traffic patterns in large data centers are highly predictable (following a "gravity model"), building a network for the absolute worst-case scenario is inefficient.
\begin{itemize}
    \item Google has introduced \textbf{Optical Circuit Switches (OCS)} into its Jupiter architecture. These devices use MEMS mirrors to steer light and create direct, high-bandwidth optical circuits between aggregation blocks.
    \item This creates a dynamic, reconfigurable mesh topology. The rigid spine layer is removed, and direct connectivity is adapted in real-time to match application communication patterns. This allows for rapid fabric expansion and seamless integration of new hardware, as new aggregation blocks can simply be plugged into the optical fabric.
\end{itemize}

\section{Addressing and Routing in Large-Scale DCNs}
The massive scale of modern DCNs makes traditional routing protocols unworkable.
\begin{itemize}
    \item \textbf{Layer-2 Limitations:} A single Layer-2 domain is not viable due to enormous forwarding tables, broadcast storms (ARP), and the inherent issues of Spanning Tree Protocol (STP), which blocks redundant paths and converges slowly.
    \item \textbf{Layer-3 Solutions:} Modern DCNs use Layer-3 routing down to the ToR switch. To manage the complexity and leverage the rich multipath fabric of a Clos network, protocols like \textbf{BGP (Border Gateway Protocol)} are used in conjunction with \textbf{Equal Cost Multiple Path (ECMP)} routing. This allows traffic to be effectively load-balanced across all available paths between the leaf switches, maximizing the utilization of the network's capacity.
\end{itemize}